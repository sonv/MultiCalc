\newpage
# Optimization

## First and second derivative tests

Read Stewart Chapter 14, Thomas Chapter 14, 

We will study multivariable scalar functions
$$ f: D \to \R \,,$$
where $D\subseteq \R^n$, $n\geq 2$.

::: {.definition}
A function $f:D \to \R$ has a **local maximum** at $\vect{x_0}$ if 
$f(\vect{x_0}) \geq f(\vect{x})$ for $\vect{x} \in B_\delta(\vect{x_0})$ for small enough $\delta$.
$f$ has a **global maximum** at $\vect{x_0}$ if 
$f(\vect{x_0}) \geq f(\vect{x})$ for $\vect{x} \in D$.
$f$ has a **local (global) minimum** at $\vect{x_0}$ if
$-f$ has a  local (global) maximum at $\vect{x_0}$
:::

::: {.theorem name="First derivative test"}
Let $f:D \to \R$ be a function.
If $\vect{x_0}$ is a local minimum and $f$ has partial derivatives at $\vect{x_0}$.
Then
\begin{equation*}
    \partial_{x_i} f(\vect{x}_0) = 0 \,.
\end{equation*}
:::

The converse is not true, as having $\nabla f(\vect{x}_0) = \vect{0}$ does not mean
that $f$ has a local minimum at $\vect{x}_0$.

:::{.exercise}
Think of a function that the converse to the above theorem is not true.
:::

This leads to the following notion.

:::{.definition}
$\vect{x}_0$ is said to be a **critical point** of $f:D\to \R$ if
\begin{equation*}
    \nabla f(\vect{x}_0) = 0
\end{equation*}
or one of the partial derivatives $\partial_{x_i} f(\vect{x}_0)$ fails to exist.
:::

**Please pay attention about the "fail to exist" condition.**

:::{.theorem name="Second derivative test for functions of 2 variables"}
Suppose the second partial derivatives of $f$ are continuous near $(a,b)$
and suppose that $(a,b)$ is a critical point of $f$.
Let 
\begin{equation*}
    D = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2\,.
\end{equation*}

1. If $D>0$ and $f_{xx}(a,b) >0$, then $f(a,b)$ is a local minimum.

2. If $D>0$ and $f_{xx}(a,b) <0$, then $f(a,b)$ is a local maximum.

3. If $D<0$, then $f(a,b)$ is neither a local maximum nor local minimum.

4. If $D=0$, then we cannot conclude.
:::

:::{.theorem name="Extreme value theorem"}
If $f$ is continuous on a _closed_ and _bounded_ set $D$. Then,
$f$ attains an absolute minimum and an absolute maximum in $D$.
:::

### Algorithm to find absolute maxima and minima on closed bounded regions

1. Find the values of $f$ at the critical points of $f$ in $D$.

2. Find the extreme values of $f$ on the boundary of $D$.

3. The largest of the values from steps 1 and 2 is the absolute maximum value;
   the smallest of these values is the absolute minimum value.


## Constrained optimization

Constrained optimization takes various forms, depending on the assumptions.
We will deal with the most straight forward form. 
The problem we will study is the following:

Maximize/minimize a function $f:D\to \R$, subject to a constraint (side condition)
of the form
$g(\vect{x}) = k$, for some constant $k\in \R$.

:::{.theorem name="Method of Lagrange Multiplier"}
Suppose the maximum/minimum values of $f$ exist and $\nabla g(\vect{x}) \not=0$ where $g(\vect{x}) = k$.
To find the maximum and minimum values of $f$ subject to constraint
$g(\vect{x}) = k$, we do the following:

1. Find all values of $\vect{x}$ and $\lambda \in \R$ such that
\begin{equation*}
    \nabla f(\vect{x}) =\lambda \nabla g(\vect{x})\,,
\end{equation*}
and
\begin{equation*}
    g(\vect{x}) = k \,.
\end{equation*}

2. Evaluate $f$ at all the points $\vect{x}$ that result from step 1. The largest of
   these values is the maximum of $f$; the smallest is the minimum value of $f$.
:::
